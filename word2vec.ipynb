{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding of Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2013년 토마스 미콜로프 등이 제안한 워드투벡(Word2Vec)은 자연어처리에서 가장 영향력이 있는 딥러닝 기술 중 하나이다. \n",
    "이 기술은 대규모 텍스트 데이터셋을 사용하여 단어를 벡터(임베딩)로 변환한다.\n",
    "이 벡터 표현은 감정 분류와 같은 하위 태스크에 사용된다.\n",
    "\n",
    "아래는 워드투벡이 어떻게 단어를 벡터로 바꾸는지에 대한 예시이다.\n",
    "$$vec(왕)=\\begin{bmatrix}-2.1&4.1&0.6\\end{bmatrix}$$\n",
    "$$vec(여왕)=\\begin{bmatrix}-1.9&2.6&1.5\\end{bmatrix}$$\n",
    "$$vec(남자)=\\begin{bmatrix}3.0&-1.1&-1\\end{bmatrix}$$\n",
    "$$vec(여자)=\\begin{bmatrix}2.8&-2.6&-1.1\\end{bmatrix}$$\n",
    "\n",
    "이 예제에서 유클리드 거리로 측정했을 때 왕과 여왕 사이의 거리(4.37)가 왕과 여자 사이의 거리(8.47)보다 가까운 것을 알 수 있다. \n",
    "\n",
    "워드투벡의 놀라운 결과 중 하나는 유추를 할 수 있다는 것이다.\n",
    "\n",
    "대표적인 예는 \"남자는 여자와 대응한다면, 왕은 무엇과 대응하는가?\"라는 질문이다.\n",
    "이는 아래와 같이 계산될 수 있다.\n",
    "$$vec(왕)-vec(남자)+vec(여자) \\approx vec(여왕)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW VS Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 벡터 표현을 만들기 위해 모델은 사전 작업을 수행해야 한다.\n",
    "\n",
    "저자는 비슷한 작업을 하는 두 가지 구조를 제안했다.\n",
    "\n",
    "* CBOW(Continuous Bag-Of-Words) 모델: 주변 맥락(대상 단어의 앞과 뒤에 오는 단어)을 활용하여 단어를 예측하도록 훈련된다. 주변 단어들 임베딩의 합을 사용하기 때문에 주변 단어들이 순서는 중요하지 않다. 저자는 예측되는 단어의 앞, 뒤 네 단어를 사용하는 것의 성능이 좋다고 언급했다.\n",
    "* Skip-Gram 모델: 단어 하나를 모델에 넣고 주변 단어들을 예측한다. 주변 단어의 범위가 넓어질수록 좋은 임베딩을 얻을 수 있지만 학습 시간이 증가한다.\n",
    "\n",
    "※ 일반적으로 CBOW 모델은 학습할 때 더 빠르고 Skip-Gram 모델은 자주 나오지 않는 단어를 학습하는 능력 덕분에 더 정확하다고 알려져 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image #pip install IPython\n",
    "Image(\"image/cbow_vs_skipgram.png\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemention of Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스킵그램은 (대상 다넝, 맥락 단어)의 쌍으로 표현할 수 있다.\n",
    "\n",
    "이때, 대상 단어는 입력이고 맥락 단어는 예측할 단어이다. \n",
    "\n",
    "아래 표와 같이 같은 대상이라도 맥락 크기에 따라서 스킵그램 수가 달라진다.\n",
    "\n",
    "|맥락 크기|텍스트|스킵그램|\n",
    "|------|---|---|\n",
    "|1|**the train** was late|('the', 'train')|\n",
    "|1|**the train was** late|('train', 'the'), ('train', 'was')|\n",
    "|1|the **train was late**|('was', 'train'), ('was', 'late')|\n",
    "|1|the train **was late**|('late', 'was')|\n",
    "|2|**the train** was late|('the', 'train'), ('the', 'was')|\n",
    "|2|**the train was late**|('train', 'the'), ('train', 'was'), ('train', 'late')|\n",
    "|2|**the train was late**|('was', 'the'), ('was', 'train'), ('was', 'late')|\n",
    "|2|the **train was late**|('late', 'train'), ('late', 'was')|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Input, Embedding, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits, word_freqs):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "def merge_pair(a, b, splits, word_freqs):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "def wordpiece_tokenizer(corpus, vocab_size):\n",
    "    print(\"Loading Tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    word_freqs = defaultdict(int)\n",
    "    alphabet, vocab = [], []\n",
    "\n",
    "    print(\"Initialize alphabet...\")\n",
    "    # append alphabet\n",
    "    for text in corpus:\n",
    "        words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "        new_words = [word for word, offset in words_with_offsets]\n",
    "        for word in new_words:\n",
    "            word_freqs[word] += 1\n",
    "\n",
    "    for word in word_freqs.keys():\n",
    "        if word[0] not in alphabet:\n",
    "            alphabet.append(word[0])\n",
    "        for letter in word[1:]:\n",
    "            if f\"##{letter}\" not in alphabet:\n",
    "                alphabet.append(f\"##{letter}\")\n",
    "\n",
    "    print(\"Constructing vocabulary...\")\n",
    "\n",
    "    # append vocab\n",
    "    vocab = vocab + alphabet.copy()\n",
    "\n",
    "    splits = {\n",
    "        word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "        for word in word_freqs.keys()\n",
    "    }\n",
    "    while len(vocab) < vocab_size:\n",
    "        scores = compute_pair_scores(splits, word_freqs)\n",
    "        best_pair, max_score = \"\", None\n",
    "        for pair, score in scores.items():\n",
    "            if max_score is None or max_score < score:\n",
    "                best_pair = pair\n",
    "                max_score = score\n",
    "        splits = merge_pair(*best_pair, splits, word_freqs)\n",
    "        new_token = (\n",
    "            best_pair[0] + best_pair[1][2:]\n",
    "            if best_pair[1].startswith(\"##\")\n",
    "            else best_pair[0] + best_pair[1]\n",
    "        )\n",
    "        vocab.append(new_token)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def encode_word(word, vocab):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens\n",
    "\n",
    "def tokenize(text, vocab):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word, vocab) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "def skipgram_input_generator(text, context_size):\n",
    "    skipgrams = []\n",
    "    for i in range(context_size, len(text) - context_size):\n",
    "        array = [text[j] for j in np.arange(i - context_size, i + context_size + 1) if j != i]\n",
    "        skipgrams.append((text[i], array))\n",
    "\n",
    "    return skipgrams\n",
    "\n",
    "def negative_sampling(df, n_samples=5):\n",
    "    unique_input_2 = df[\"input_2\"].unique()\n",
    "    negative_samples = []\n",
    "    \n",
    "    for input_1 in df[\"input_1\"].unique():\n",
    "        # 해당 input_1에 대한 기존 positive input_2 값\n",
    "        existing_input_2 = df[df[\"input_1\"] == input_1][\"input_2\"].tolist()\n",
    "        \n",
    "        # 네거티브 샘플 후보에서 기존 input_2 제외\n",
    "        negative_candidates = list(set(unique_input_2) - set(existing_input_2))\n",
    "        \n",
    "        # 네거티브 샘플 추출\n",
    "        sampled_negatives = np.random.choice(negative_candidates, n_samples, replace=True)\n",
    "        \n",
    "        # 결과 추가\n",
    "        for neg in sampled_negatives:\n",
    "            negative_samples.append({\"input_1\": input_1, \"input_2\": neg, \"output\": 0})\n",
    "    \n",
    "    # 네거티브 샘플을 데이터프레임으로 변환\n",
    "    return pd.DataFrame(negative_samples)\n",
    "\n",
    "def preprocessing_data(df, TRAIN_SIZE): # Split data into 8/2\n",
    "    df = shuffle(df, random_state=1)\n",
    "    cutoff = int(TRAIN_SIZE * len(df))\n",
    "    train_df = df.iloc[:cutoff]\n",
    "    test_df = df.iloc[cutoff:]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def skipgram(word_num, dims):\n",
    "    word_input_1 = Input(shape=(1,), dtype = \"int32\", name = \"word_input_1\")\n",
    "    word_embedding_1 = Embedding(word_num, dims, input_length = word_input_1.shape[1], name = \"input_1_Embedding\")(word_input_1)\n",
    "    word_embedding_1 = Flatten(name = \"input_1_Flatten\")(word_embedding_1)\n",
    "\n",
    "    word_input_2 = Input(shape=(1,), dtype = \"int32\", name = \"word_input_2\")\n",
    "    word_embedding_2 = Embedding(word_num, dims, input_length = word_input_2.shape[1], name = \"input_2_Embedding\")(word_input_2)\n",
    "    word_embedding_2 = Flatten(name = \"input_2_Flatten\")(word_embedding_2)\n",
    "\n",
    "    input_12 = Concatenate(name = \"Concat_input_1_input_2\")([word_embedding_1, word_embedding_2])\n",
    "    outputs = Dense(1, activation = \"sigmoid\", name = \"outputs\")(input_12)\n",
    "\n",
    "    model = Model(inputs = [word_input_1, word_input_2], outputs = outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc eu sem \n",
    "scelerisque, dictum eros aliquam, accumsan quam. Pellentesque tempus, lorem ut \n",
    "semper fermentum, ante turpis accumsan ex, sit amet ultricies tortor erat quis \n",
    "nulla. Nunc consectetur ligula sit amet purus porttitor, vel tempus tortor \n",
    "scelerisque. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices \n",
    "posuere cubilia curae; Quisque suscipit ligula nec faucibus accumsan. Duis \n",
    "vulputate massa sit amet viverra hendrerit. Integer maximus quis sapien id \n",
    "convallis. Donec elementum placerat ex laoreet gravida. Praesent quis enim \n",
    "facilisis, bibendum est nec, pharetra ex. Etiam pharetra congue justo, eget \n",
    "imperdiet diam varius non. Mauris dolor lectus, interdum in laoreet quis, \n",
    "faucibus vitae velit. Donec lacinia dui eget maximus cursus. Class aptent taciti\n",
    "sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Vivamus\n",
    "tincidunt velit eget nisi ornare convallis. Pellentesque habitant morbi \n",
    "tristique senectus et netus et malesuada fames ac turpis egestas. Donec \n",
    "tristique ultrices tortor at accumsan.\n",
    "\"\"\"\n",
    "corpus = text.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem ipsum dolor sit amet, consectetur adipiscing elit',\n",
       " ' Nunc eu sem \\nscelerisque, dictum eros aliquam, accumsan quam',\n",
       " ' Pellentesque tempus, lorem ut \\nsemper fermentum, ante turpis accumsan ex, sit amet ultricies tortor erat quis \\nnulla',\n",
       " ' Nunc consectetur ligula sit amet purus porttitor, vel tempus tortor \\nscelerisque',\n",
       " ' Vestibulum ante ipsum primis in faucibus orci luctus et ultrices \\nposuere cubilia curae; Quisque suscipit ligula nec faucibus accumsan']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Corpus (using WordPiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer...\n",
      "Initialize alphabet...\n",
      "Constructing vocabulary...\n"
     ]
    }
   ],
   "source": [
    "vocab = wordpiece_tokenizer(corpus, 100)\n",
    "tokenized_text = tokenize(text, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "skipgrams = skipgram_input_generator(tokenized_text, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('##m', ['Lor', '##e', 'ip', '##s']), ('ip', ['##e', '##m', '##s', '##u'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgrams[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Word2Vec Dataset(using negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_df = pd.DataFrame(columns=[\"input_1\", \"input_2\", \"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_1</th>\n",
       "      <th>input_2</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [input_1, input_2, output]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 841/841 [00:01<00:00, 579.20it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(skipgrams))):\n",
    "    tuple_ = skipgrams[i]\n",
    "    input1 = tuple_[0]\n",
    "    input2_list = tuple_[1]\n",
    "    for j in range(len(input2_list)):\n",
    "        word2vec_df = pd.concat([word2vec_df,\n",
    "                                 pd.DataFrame([{\n",
    "                                     \"input_1\": input1,\n",
    "                                     \"input_2\": input2_list[j],\n",
    "                                     \"output\": 1\n",
    "                                 }])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_1</th>\n",
       "      <th>input_2</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##m</td>\n",
       "      <td>Lor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##m</td>\n",
       "      <td>##e</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##m</td>\n",
       "      <td>ip</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##m</td>\n",
       "      <td>##s</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ip</td>\n",
       "      <td>##e</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input_1 input_2 output\n",
       "0     ##m     Lor      1\n",
       "0     ##m     ##e      1\n",
       "0     ##m      ip      1\n",
       "0     ##m     ##s      1\n",
       "0      ip     ##e      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 네거티브 샘플 추가\n",
    "negative_df = negative_sampling(word2vec_df, n_samples=5)\n",
    "\n",
    "# 기존 데이터와 병합\n",
    "result_df = pd.concat([word2vec_df, negative_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_1</th>\n",
       "      <th>input_2</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##c</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>##l</td>\n",
       "      <td>convall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##u</td>\n",
       "      <td>##i</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##m</td>\n",
       "      <td>##t</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>ornar</td>\n",
       "      <td>##r</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    input_1  input_2 output\n",
       "0       ##c        a      1\n",
       "97      ##l  convall      0\n",
       "0       ##u      ##i      1\n",
       "0       ##m      ##t      1\n",
       "328   ornar      ##r      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value of input_1: 68\n",
      "max value of input_2: 68\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "# input_1과 input_2의 값을 합쳐 고유값 추출\n",
    "combined_values = pd.concat([result_df[\"input_1\"], result_df[\"input_2\"]], axis=0).unique()\n",
    "\n",
    "# 고유값 라벨 인코딩\n",
    "le.fit(combined_values)\n",
    "\n",
    "# input_1과 input_2를 라벨 인코딩\n",
    "result_df[\"input_1\"] = le.transform(result_df[\"input_1\"])\n",
    "result_df[\"input_2\"] = le.transform(result_df[\"input_2\"])\n",
    "print(f\"max value of input_1: {result_df['input_1'].max()}\")\n",
    "print(f\"max value of input_2: {result_df['input_2'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_1</th>\n",
       "      <th>input_2</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input_1  input_2 output\n",
       "0        8       25      1\n",
       "0        8        4      1\n",
       "0        8       50      1\n",
       "0        8       14      1\n",
       "0       50        4      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train dataset: 2963\n",
      "The length of test dataset: 741\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = preprocessing_data(result_df, 0.8)\n",
    "print(f\"The length of train dataset: {len(train_df)}\")\n",
    "print(f\"The length of test dataset: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_1 = train_df[\"input_1\"].values\n",
    "train_input_2 = train_df[\"input_2\"].values\n",
    "train_output = train_df[\"output\"].values\n",
    "\n",
    "test_input_1 = test_df[\"input_1\"].values\n",
    "test_input_2 = test_df[\"input_2\"].values\n",
    "test_output = test_df[\"output\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " word_input_1 (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " word_input_2 (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_1_Embedding (Embeddi  (None, 1, 100)               6800      ['word_input_1[0][0]']        \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " input_2_Embedding (Embeddi  (None, 1, 100)               6800      ['word_input_2[0][0]']        \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " input_1_Flatten (Flatten)   (None, 100)                  0         ['input_1_Embedding[0][0]']   \n",
      "                                                                                                  \n",
      " input_2_Flatten (Flatten)   (None, 100)                  0         ['input_2_Embedding[0][0]']   \n",
      "                                                                                                  \n",
      " Concat_input_1_input_2 (Co  (None, 200)                  0         ['input_1_Flatten[0][0]',     \n",
      " ncatenate)                                                          'input_2_Flatten[0][0]']     \n",
      "                                                                                                  \n",
      " outputs (Dense)             (None, 1)                    201       ['Concat_input_1_input_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13801 (53.91 KB)\n",
      "Trainable params: 13801 (53.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "skipgram = skipgram(68, 100)\n",
    "skipgram.summary()\n",
    "\n",
    "adam = Adam(learning_rate=0.0001)\n",
    "\n",
    "skipgram.compile(optimizer=adam, loss=BinaryCrossentropy)\n",
    "es = EarlyStopping(monitor='val_loss', mode = 'min', verbose = 1, patience = 5, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = skipgram.fit([train_input_1, train_input_2],\n",
    "              train_output,\n",
    "              batch_size = 32,\n",
    "              epochs = 50,\n",
    "              callbacks=[es],\n",
    "              validation_split = 0.125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = (skipgram.predict([test_input_1, test_input_2]) >= 0.5).astype(int)\n",
    "\n",
    "# 모델 성능 평가 지표 계산 및 출력\n",
    "metrics = {\n",
    "    \"Precision\": precision_score(test_output, y_pred_binary) * 100,\n",
    "    \"Recall\": recall_score(test_output, y_pred_binary) * 100,\n",
    "    \"F1_Score\": f1_score(test_output, y_pred_binary) * 100,\n",
    "    \"Accuracy\": accuracy_score(test_output, y_pred_binary) * 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: {round(metrics[\"Precision\"], 3)}\")\n",
    "print(f\"Recall: {round(metrics[\"Recall\"], 3)}\")\n",
    "print(f\"F1_Score: {round(metrics[\"F1_Score\"], 3)}\")\n",
    "print(f\"Accuracy: {round(metrics[\"Accuracy\"], 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "free",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
